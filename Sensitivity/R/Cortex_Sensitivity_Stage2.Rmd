---
title: "Analyses: CORTEX-D-19-00912"
subtitle: "<h2><u>Cortex Ecological Risk RegReport</u></h2>"
author: "<h3>Demidenko, Ip, Kelly,  Goetschius, Constante, Keating</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
tags: [Cortex, ABCD, Risk, Parenting, Amygdala] 
output:
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_folding: show
    self_contained: yes
---

For the script to correctly select files and combine by `event_name` and `subjectkey` into a single merged dataset, ensure that the `ABCDStudyNDA_1182451` associated with the `https://nda.nih.gov/study.html?id=901` 3.0 release. 

Install all **packages required to run** below code, if not already installed.
```{r message=FALSE, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(janitor, cowplot, dplyr, readr, devtools, table1, MatchIt, descr, car, kableExtra, corrplot, multiplex, MplusAutomation)
```


Function from `Mike Angstat` to combined .txt data files from downloaded pre-packaged data.
https://github.com/mangstad/Misc_utils/blob/master/abcd_functions.R
```{r eval=FALSE, include=FALSE}
read.abcd = function(file, sep="\t",skip=1,cols=NULL,descriptions=FALSE) {
  headers = names(read.table(file,sep=sep,header=T,stringsAsFactors=F)[-1,])
  if (descriptions) {
    descrip = names(read.table(file,sep=sep,header=T,stringsAsFactors=F,skip=1))
  }
  data = read.table(file,sep=sep,header=T,stringsAsFactors=F,skip=skip)
  names(data) = headers
  if (!is.null(cols)) {
    data = subset(data,select=cols)
  }
  if (descriptions) {
    list(data=data,descrip=descrip)
  } else {
    data
  }
}

multi.merge = function(...,by=NULL) {
  Reduce(function(x,y) merge(x,y, all=TRUE,by=by), list(...))
}

```

Combining .txt files from pre-packaged data. Based on data dictionary info at:
https://nda.nih.gov/data_dictionary.html?source=ABCD%2BRelease%2B3.0&submission=ALL
```{r eval=FALSE, include=FALSE}
library(tidyverse)

#merging by
mergecols = c("subjectkey","eventname")

# select internalizing
cbcl = read.abcd("ABCDStudyNDA_1182451/abcd_cbcls01.txt") %>% 
  select(subjectkey,eventname,cbcl_scr_syn_internal_t)

# select income
income_base = read.abcd("ABCDStudyNDA_1182451/pdem02.txt") %>% 
  select(subjectkey,eventname,demo_comb_income_v2)

income_yr1 = read.abcd("ABCDStudyNDA_1182451/abcd_lpds01.txt") %>%
  select(subjectkey,eventname,demo_comb_income_v2_l)


# select MRI manufact
mri = read.abcd("ABCDStudyNDA_1182451/abcd_mri01.txt")%>%
  select(subjectkey,eventname,mri_info_manufacturer)


# select Par Safe Q
par_safe = read.abcd("ABCDStudyNDA_1182451/abcd_pnsc01.txt") %>%
  select(subjectkey,eventname,
         neighborhood1r_p, 
         neighborhood2r_p, 
         neighborhood3r_p) 


# select ADI
redhist = read.abcd("ABCDStudyNDA_1182451/abcd_rhds01.txt") %>%
  select(subjectkey,eventname,
         reshist_addr1_adi_wsum)

# select puberty
pubert = read.abcd("ABCDStudyNDA_1182451/abcd_ssphp01.txt") %>%
  select(subjectkey,eventname,
         pds_p_ss_female_category_2,
         pds_p_ss_male_category_2)

# select caregiver acceptance
accept1 = read.abcd("ABCDStudyNDA_1182451/crpbi01.txt") %>%
  select(subjectkey,eventname,
         crpbi_studycaregiver_id) # who caregiver is
accept2 = read.abcd("ABCDStudyNDA_1182451/abcd_sscey01.txt") %>%
  select(subjectkey,eventname,
         crpbi_acceptance_ss_studycaregiver = crpbi_y_ss_parent)


# rel_fam & race
demograph = read.abcd("ABCDStudyNDA_1182451/acspsw03.txt") %>%
  select(subjectkey,eventname,
         interview_age,
         sex,# M = Male; F = Female; O=Other; NR = Not reported
         race_ethnicity, #1 = White; 2 = Black; 3 = Hispanic; 4 = Asian; 5 = Other
         rel_family_id
         )

# site id
site = read.abcd("ABCDStudyNDA_1182451/abcd_lt01.txt") %>% 
  select(subjectkey,eventname,
         interview_age,
         abcd_site = site_id_l)


# MRI /qC variables - sensitivity per Cosnortium recommendations (Repronim)

nBack_fmri_raw = read.abcd("ABCDStudyNDA_1182451/mriqcrp202.txt") %>% 
  select(subjectkey,eventname, iqc_nback_ok_ser)#nBack: Number of series that are complete and passed QC

nBack_rawqc = read.abcd("ABCDStudyNDA_1182451/mriqcrp102.txt") %>% #
  select(subjectkey,eventname, iqc_t1_ok_ser)#T1: Number of series that are complete and passed QC

n_back_perf = read.abcd("ABCDStudyNDA_1182451/=abcd_mrinback02.txt") %>%
  select(subjectkey,eventname,
         tfmri_nback_beh_performflag, # 2back & all back corr >= .60
         tfmri_nb_all_beh_ctotal_mrt, # mean response times for all corr responses
         tfmri_nb_all_beh_ctotal_stdrt) # std of reaction time for all corr responses

nback_brain = read.abcd("ABCDStudyNDA_1182451/=nback_bwroi02.txt") %>%
  select(subjectkey,eventname,
         tfmri_nback_all_face.vs.place_beta_subcort.aseg_amygdala.lh = tfmri_nback_all_164,
         tfmri_nback_all_face.vs.place_beta_subcort.aseg_amygdala.rh = tfmri_nback_all_178, 
         tfmri_nback_all_beta_mm, # Average framewise displacement in mm
         tfmri_nback_all_beta_dof) # Degrees of freedom


nback_trials = read.abcd("ABCDStudyNDA_1182451/abcd_mrinback02.txt") %>% 
  select(subjectkey,eventname,
         tfmri_nb_all_beh_total_nt)# Total number of trials	

nback_eprime = read.abcd("ABCDStudyNDA_1182451/mriqcrp302.txt") %>% 
  select(subjectkey,eventname,
         iqc_nback_ep_t_series_match, # (-1 not found; 0 no match; 1 match)
         eprime_mismatch_ok_nback)# Ignore difference between time of nBack image series and nBack E-prime file


fmri_post_autoqc = read.abcd("ABCDStudyNDA_1182451/abcd_auto_postqc01.txt") %>% 
  select(subjectkey,eventname,
         apqc_fmri_bounwarp_flag, # Scans available for fMRI B0 unwarp
         apqc_fmri_regt1_rigid,
         apqc_fmri_fov_cutoff_ventral) # maximum ventral cutoff score in fmri fm images
         fmri_postqc_qc) # overall QC score (0, no disparity, disparity btwn raters)


fsqc = read.abcd("ABCDStudyNDA_1182451/freesqc01.txt") %>% 
  select(subjectkey,eventname,
         fsqc_qc) # freesurfer qc pass/fail

fmri_post_manual = read.abcd("ABCDStudyNDA_1182451/fmriqc01.txt") %>% 
  select(subjectkey,eventname,
         fmri_postqc_qc) # overall QC score (0, no disparity, disparity btwn raters)

         


# Merge all above by site/id
abcd = multi.merge(demograph,site,income_base, income_yr1,cbcl,
                   mri, par_safe,
                   redhist, pubert,accept1,accept2,
                   nBack_fmri_raw, nBack_rawqc, 
                   n_back_perf, nback_brain, nback_trials, nback_eprime, 
                   fmri_post_autoqc, fsqc,fmri_post_manual,
                   by=mergecols)

abcd = abcd %>% 
  filter(eventname == "baseline_year_1_arm_1" | eventname == "1_year_follow_up_y_arm_1") %>% 
  mutate(event_name = if_else(eventname == "baseline_year_1_arm_1", "Baseline", "Year 1"))


# write file
write.csv(abcd, "Release3_2020_11_25_SENSITIVITY.csv")

```





                          
                                  
# **Loading**

## Loading .csv data
Start here to reduce time in loading large .rds file

```{r message=FALSE, warning=FALSE}
library(tidyverse)

# load data that wsa written above
abcd = read_csv("Release3_2020_11_25_SENSITIVITY.csv") %>% select(-X1)
#abcd = read_csv("~/Desktop/UM/4_ABCD/3_Projects/Cortex_sub/Stage2/data/data_release3_2020_11_04.csv") %>% select(-X1)

abcd = abcd %>%
  rename(subj_id = subjectkey,
         interview_age = interview_age.x,
         MRI_QC_general = fsqc_qc, # free surfer QC Accept/Reject
         nback_meanFD_motion = tfmri_nback_all_beta_mm, #nback Mean framewise displacement in mm
         Nback_beh_60 = tfmri_nback_beh_performflag, # participant acceptable performance task; *corr.2.back_rate >= 0.6 & *beh_corr.0.back_rate >= 0.6
         Right_amyg_nback = tfmri_nback_all_face.vs.place_beta_subcort.aseg_amygdala.rh,
         Left_amyg_nback = tfmri_nback_all_face.vs.place_beta_subcort.aseg_amygdala.lh,
         Nback_RAW_qc = iqc_nback_ok_ser,
         T1_RAW_qc = iqc_t1_ok_ser,
         nback_DOF = tfmri_nback_all_beta_dof,
         nback_TRIALS = tfmri_nb_all_beh_total_nt,
         EPrime_match1 = iqc_nback_ep_t_series_match,
         EPrime_match2 = eprime_mismatch_ok_nback,
         B0_avail = apqc_fmri_bounwarp_flag,
         fMRI_manual_qc = fmri_postqc_qc,
         T1w_reg = apqc_fmri_regt1_rigid,
         Ventral_cut = apqc_fmri_fov_cutoff_ventral,
         ethnicity = race_ethnicity,
         neighb_phenx_1r_p = neighborhood1r_p,
         neighb_phenx_2r_p = neighborhood2r_p,
         neighb_phenx_3r_p = neighborhood3r_p, 
         tfmri_nback_all_face.vs.place_beta_subcort.aseg_amygdala.lh,
         tfmri_nback_all_face.vs.place_beta_subcort.aseg_amygdala.rh,          
         pubertdev_ss_male_category_p = pds_p_ss_male_category_2 , 
         pubertdev_ss_female_category_p = pds_p_ss_female_category_2) %>% 
  select(-c(interview_age.y, eventname))
```


# **Data clean** 
using similar methods as in:

## Updating factor variables 
```{r message=FALSE, warning=FALSE, include=FALSE}

abcd$income_base_f = factor(abcd$demo_comb_income_v2, 
                            levels = c(1,2,3,4,5,6,7,8,9,10,777,999),
                            labels = c('Less than $5,000',
                                       '$5,000 through $11,999',
                                       '$12,000 through $15,999',
                                       '$16,000 through $24,999',
                                       '$25,000 through $34,999',
                                       '$35,000 through $49,999',
                                       '$50,000 through $74,999',
                                       '$75,000 through $99,999',
                                       '$100,000 through $199,999',
                                       '$200,000 and greater', 
                                       'Refuse to answer',
                                       "Don't Know"))

abcd$income_yr1_f = factor(abcd$demo_comb_income_v2_l, 
                            levels = c(1,2,3,4,5,6,7,8,9,10,777,999),
                            labels = c('Less than $5,000',
                                       '$5,000 through $11,999',
                                       '$12,000 through $15,999',
                                       '$16,000 through $24,999',
                                       '$25,000 through $34,999',
                                       '$35,000 through $49,999',
                                       '$50,000 through $74,999',
                                       '$75,000 through $99,999',
                                       '$100,000 through $199,999',
                                       '$200,000 and greater',
                                       'Refuse to answer',
                                       "Don't Know"))

# recoding factor income to numeric BASELINE
abcd = abcd %>% 
  mutate(income_base_r = case_when(income_base_f == 'Less than $5,000' ~ 1,
                            income_base_f == '$5,000 through $11,999' ~ 2,
                            income_base_f == '$12,000 through $15,999' ~ 3,
                            income_base_f == '$16,000 through $24,999' ~ 4,
                            income_base_f == '$25,000 through $34,999' ~ 5,
                            income_base_f == '$35,000 through $49,999' ~ 6,
                            income_base_f == '$50,000 through $74,999' ~ 7,
                            income_base_f == '$75,000 through $99,999' ~ 8,
                            income_base_f == '$100,000 through $199,999' ~ 9,
                            income_base_f == '$200,000 and greater' ~ 10,
                            TRUE ~ NA_real_))

abcd %>% 
  distinct(abcd_site)
abcd = abcd %>% 
  mutate(site = case_when(abcd_site == 'site01' ~ 1,
                          abcd_site == 'site02' ~ 2,
                          abcd_site == 'site03' ~ 3,
                          abcd_site == 'site04' ~ 4,
                          abcd_site == 'site05' ~ 5,
                          abcd_site == 'site06' ~ 6,
                          abcd_site == 'site07' ~ 7,
                          abcd_site == 'site08' ~ 8,
                          abcd_site == 'site09' ~ 9,
                          abcd_site == 'site10' ~ 10,
                          abcd_site == 'site11' ~ 11,
                          abcd_site == 'site12' ~ 12,
                          abcd_site == 'site13' ~ 13,
                          abcd_site == 'site14' ~ 14,
                          abcd_site == 'site15' ~ 15,
                          abcd_site == 'site16' ~ 16,
                          abcd_site == 'site17' ~ 17,
                          abcd_site == 'site18' ~ 18,
                          abcd_site == 'site19' ~ 19,
                          abcd_site == 'site20' ~ 20,
                          abcd_site == 'site21' ~ 21,
                          abcd_site == 'site22' ~ 22,
                          TRUE ~ NA_real_))


abcd %>% 
  distinct(mri_info_manufacturer)
abcd = abcd %>% 
  mutate(scanner_type = case_when(mri_info_manufacturer == 'SIEMENS' ~ 1,
                                  mri_info_manufacturer == 'GE MEDICAL SYSTEMS' ~ 2,
                                  mri_info_manufacturer == 'Philips Medical Systems' ~ 3,
                                  TRUE ~ NA_real_))
                                  

# coding sex variable from factor to numeric
abcd = abcd %>% 
  mutate(sex_r = case_when(sex == 'M' ~ 1,
                            sex == 'F' ~ 0))

# race
abcd$race5 = factor(abcd$ethnicity, 
                            levels = c(1,2,3,4,5),
                            labels = c("White", "Black", "Hispanic", "Asian", "Other"))


# self-report measure of PUBERTAL STATUS  [1] prepuberty ; [2] - early puberty ; [3]-mid puberty; [4] - late puberty; [5] - post puberty
abcd = abcd %>% 
  mutate(puberty = case_when(sex == 'F' ~ pubertdev_ss_female_category_p,
                             sex == 'M' ~ pubertdev_ss_male_category_p))

# Create parent self-report avg, which is 1+2+3/3
abcd = abcd %>% 
  mutate(neighb_phenx_ss_mean_p = (neighb_phenx_1r_p + neighb_phenx_2r_p + neighb_phenx_3r_p)/3)


# Create average of Bilateral amyg activation
abcd = abcd %>% 
  mutate(Bilat_Amyg = ((Right_amyg_nback + Left_amyg_nback) /2))


```


## Conducting QC checks

```{r message=TRUE, warning=TRUE}

abcd_baseline = abcd %>% 
  filter(event_name =="Baseline")
abcd_baseline = abcd_baseline[!duplicated(abcd_baseline),]

abcd_yr1 = abcd %>% 
  filter(event_name =="Year 1")

abcd = as.data.frame(dplyr::bind_rows(abcd_baseline, abcd_yr1))

rm(abcd_yr1, abcd_baseline)

# reduce YR1 subject N to match those in BASELINE
# creating variable to match N in BL to YR1
subj_BL_ids <- abcd %>% 
  filter(event_name == "Baseline") %>% 
  distinct(subj_id)

library(janitor) # to get_dupes() in BASELINE
subj_BL_dupl <- abcd %>% 
  filter(event_name == "Baseline") %>% 
  get_dupes(subj_id)

# which rows to remove
abcd[abcd$subj_id =="NDAR_INV2ZA2LC3N",][1]
abcd[abcd$subj_id =="NDAR_INV3E0WVH3G",][1]
abcd[abcd$subj_id =="NDAR_INVJ9GNXGK5",][1]
abcd[abcd$subj_id =="NDAR_INVWE1DE80Z",][1]
abcd[abcd$subj_id =="NDAR_INVXN6HMGK8",][1]

# Based on redudant baseline in subj_BL_dupl, remove rows
abcd <- abcd[-c(1134,#NDAR_INV2ZA2LC3N
                1313,#NDAR_INV3E0WVH3G
                6807,#NDAR_INVJ9GNXGK5
                10590,#NDAR_INVWE1DE80Z
                11059),]#NDAR_INVXN6HMGK8

# confirm no more duplicates
subj_BL_dupl_test <- abcd %>% 
  filter(event_name == "Baseline") %>% 
  get_dupes(subj_id)

abcd <- subset(abcd, subj_id %in% subj_BL_ids$subj_id)
```


## ## Creating exclusion variable

Based on consortium sec

```{r message=FALSE, warning=FALSE}
rm(subj_BL_dupl, subj_BL_dupl_test)
# excluding subjects (1 = EXCLUDE; 0 = INCLUDE)
abcd = abcd %>% 
  mutate(exclude = case_when(event_name == "Year 1" ~ 0,
                             Nback_RAW_qc == 0 ~ 1,
                             Nback_RAW_qc = is.na(Nback_RAW_qc) ~ 1,
                             T1_RAW_qc == 0 ~ 1,
                             T1_RAW_qc = is.na(T1_RAW_qc) ~ 1,
                             Nback_beh_60 == 0 ~ 1,
                             Nback_beh_60 = is.na(Nback_beh_60) ~ 1,
                             nback_DOF < 200 ~ 1,
                             nback_DOF = is.na(nback_DOF) ~ 1,
                             nback_TRIALS < 100 ~1,
                             nback_TRIALS = is.na(nback_TRIALS) ~ 1,
                             EPrime_match1 == 0 ~ 1,
                             B0_avail == 0 ~ 1,
                             B0_avail = is.na(B0_avail) ~ 1,
                             MRI_QC_general == 0 ~ 1,
                             MRI_QC_general = is.na(MRI_QC_general) ~ 1,
                             fMRI_manual_qc == 1 ~ 1,
                             nback_meanFD_motion > .9 ~ 1,
                             nback_meanFD_motion = is.na(nback_meanFD_motion) ~ 1,
                             Left_amyg_nback = is.na(Left_amyg_nback) ~ 1,
                             Ventral_cut > 60 ~1,
                             Ventral_cut = is.na(Ventral_cut) ~ 1,
                             TRUE ~ 0))


abcd %>% 
  filter(event_name=="Baseline") %>% 
  group_by(exclude) %>% 
  summarise(count = n())
```


## Evaluating N ecluded
- Excluding based on QC metrics above. 
- Used in inclusion/exclusion flowchart in `Figure ##`

```{r message=FALSE, warning=FALSE}

abcd %>% 
  group_by(event_name) %>% 
  summarize(total_n = n())


# General inform/quality/time checks
abcd %>%  
  filter(event_name == "Baseline") %>% 
  group_by(Nback_RAW_qc) %>% 
  summarise('Nback pass raw QC'=n())

abcd %>%  
  filter(event_name == "Baseline") %>% 
  group_by(T1_RAW_qc) %>% 
  summarise('T1 pass raw QC'=n())

abcd %>%  
  filter(event_name == "Baseline")  %>% 
  summarise('Avg DOF'= mean(nback_DOF, na.rm = T), 'SD DOF'= sd(nback_DOF, na.rm = T))

abcd %>%  
  filter(event_name == "Baseline") %>% 
  group_by(nback_TRIALS) %>% 
  summarise('Nback Tria;s'=n())

abcd %>%  
  filter(event_name == "Baseline") %>% 
  group_by(EPrime_match1) %>% 
  summarise('Emprime data match'=n())

abcd %>%  
  filter(event_name == "Baseline") %>% 
  group_by(B0_avail) %>% 
  summarise('Fieldmap availability'=n())

abcd %>%
  filter(event_name == "Baseline") %>% 
  group_by(MRI_QC_general) %>% 
  summarise('Scan QC General'=n())
    
abcd %>%
  filter(event_name == "Baseline") %>% 
  filter(nback_meanFD_motion > .9) %>% # consider cut-off?
  summarise('Framewise Displacement Exceed FD .90'=n())

abcd %>%
  filter(event_name == "Baseline") %>% 
  group_by(Nback_beh_60) %>% 
  summarise('N-Back Beh below/above 60%'=n())


# Checking flow of participants based on exclusion values for Flowchart exclusion(s)

# Step 1: Nback Raw QC~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0) %>% 
  group_by(Nback_RAW_qc) %>% 
  summarise(Nback_Raw_QC = n()) %>% 
  summarise(nback_raw_t = sum(Nback_Raw_QC))

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc == 0 | is.na(Nback_RAW_qc)) %>% 
  group_by(Nback_RAW_qc) %>% 
  summarise(Nback_Raw_QC = n()) 

# Step 2: T1 raw QC~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0) %>%
  group_by(T1_RAW_qc) %>% 
  summarise(T1_RAW = n()) %>% 
  summarise(T1_RAW_t = sum(T1_RAW))

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc == 0 | is.na(T1_RAW_qc)) %>%
  group_by(T1_RAW_qc) %>% 
  summarise(T1_RAW = n()) 


# Step 3: Nback Behavior~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0) %>%
  group_by(Nback_beh_60) %>% 
  summarise(Nback_Perform = n()) 

# Step 4: Avg DOF~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200) %>%
  group_by(nback_DOF) %>% 
  summarise(DOF_nback = n()) %>% 
  summarise("DOF__nback_t >200" = sum(DOF_nback))

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF < 200) %>%
  group_by(nback_DOF) %>% 
  summarise(DOF_nback = n()) %>% 
  summarise("DOF__nback_t <200" = sum(DOF_nback))

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         is.na(nback_DOF)) %>%
  group_by(nback_DOF) %>% 
  summarise(DOF_nback_missing = n())

# Step 5 Nback trials ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100) %>%
  group_by(nback_TRIALS) %>% 
  summarise(Nback_trials = n()) %>% 
  summarise("Trials_total >100" = sum(Nback_trials))

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS <100) %>%
  group_by(nback_TRIALS) %>% 
  summarise(Nback_trials = n()) %>% 
  summarise("Trials_total <100" = sum(Nback_trials))

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         is.na(nback_TRIALS)) %>%
  group_by(nback_TRIALS) %>% 
  summarise(Nback_trials = n()) %>% 
  summarise("Trials Missing" = sum(Nback_trials))

# Step 6 eprime ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100) %>%
  group_by(EPrime_match1) %>% 
  summarise("Matched Eprime" = n()) 

# Step 7 B0 fieldmap ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1) %>%
  group_by(B0_avail) %>% 
  summarise("Fieldmap Available" = n()) 


# Step 8 Freesurfer QC ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1,
         B0_avail ==1) %>%
  group_by(MRI_QC_general) %>% 
  summarise("Freesurfer QC" = n()) 

# Step 9 FMRI Manuul QC ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Rater discrepant 1 = discrepant
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1,
         B0_avail ==1,
         MRI_QC_general == 1) %>%
  group_by(fMRI_manual_qc) %>% 
  summarise("fMRI Manual QC" = n()) 


# Step 10 Mean FD < .90 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Rater discrepant 1 = discrepant
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1,
         B0_avail ==1,
         MRI_QC_general == 1,
         fMRI_manual_qc == 0 | is.na(fMRI_manual_qc),
         nback_meanFD_motion > .9) %>%
  group_by(nback_meanFD_motion) %>% 
  summarise(Mean_FD_higher = n()) %>% 
  summarise("Mean FD > .90" = sum(Mean_FD_higher))

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1,
         B0_avail ==1,
         MRI_QC_general == 1,
         fMRI_manual_qc == 0 | is.na(fMRI_manual_qc),
         nback_meanFD_motion < .9) %>%
  group_by(nback_meanFD_motion) %>% 
  summarise(Mean_FD_lower = n()) %>% 
  summarise("Mean FD < .90" = sum(Mean_FD_lower))

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1,
         B0_avail ==1,
         MRI_QC_general == 1,
         fMRI_manual_qc == 0 | is.na(fMRI_manual_qc),
         is.na(nback_meanFD_motion)) %>%
  group_by(nback_meanFD_motion) %>% 
  summarise(Mean_FD_missing = n()) %>% 
  summarise("Mean FD Missing" = sum(Mean_FD_missing))

# Step 11 Ventral cutoff > 60 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1,
         B0_avail ==1,
         MRI_QC_general == 1,
         fMRI_manual_qc == 0 | is.na(fMRI_manual_qc),
         nback_meanFD_motion < .9,
         Ventral_cut > 60) %>%
  group_by(Ventral_cut) %>% 
  summarise(ventral_cut = n()) %>% 
    summarise(total_cut = sum(ventral_cut))

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1,
         B0_avail ==1,
         MRI_QC_general == 1,
         fMRI_manual_qc == 0 | is.na(fMRI_manual_qc),
         nback_meanFD_motion < .9,
         Ventral_cut < 60) %>%
  group_by(Ventral_cut) %>% 
  summarise(ventral_cut = n()) %>% 
  summarise(ventral_cut_ok = sum(ventral_cut))

# Step 12 Amyg act ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1,
         B0_avail ==1,
         MRI_QC_general == 1,
         fMRI_manual_qc == 0 | is.na(fMRI_manual_qc),
         nback_meanFD_motion < .9,
         Ventral_cut < 60,
         is.na(Left_amyg_nback)) %>%
  group_by(Left_amyg_nback) %>% 
  summarise(Left_amyg = n()) 

abcd %>% 
  filter(event_name == "Baseline",
         Nback_RAW_qc > 0,
         T1_RAW_qc > 0,
         Nback_beh_60 == 1,
         nback_DOF > 200,
         nback_TRIALS >100, 
         EPrime_match1 == 1,
         B0_avail ==1,
         MRI_QC_general == 1,
         fMRI_manual_qc == 0 | is.na(fMRI_manual_qc),
         nback_meanFD_motion < .9,
         Ventral_cut < 60,
         !is.na(Left_amyg_nback)) %>%
  group_by(Left_amyg_nback) %>% 
  summarise(Left_amyg = n()) %>% 
  summarise(Amyg_Act_Total_n = sum(Left_amyg))




# Final - total included based on criteria
abcd %>% 
  filter(event_name == "Baseline", exclude == 0) %>% # final N based in brain QC
  summarise("N_postBrain_QC" = n())
  

```

# Subset data
Subseting data based on inclusion variables created above `exclude = 0`

```{r message=FALSE, warning=FALSE}

abcd %>% 
  group_by(event_name) %>% 
  summarise(n = n())

abcd_incl = abcd %>% 
  filter(exclude == 0)


abcd_incl %>% 
  group_by(event_name) %>% 
  summarise(Event_N = n())
```




# Vis. data/var

## Env Risk Var
### Parental SR neigb  safety
mean of parent report - neighb_phenx_1r_p + neighb_phenx_2r_p + neighb_phenx_3r_p)/3 - (1-5; 5 = strongly agree) 

  - Q1 I feel safe walking in my neighborhood, day or night; neighb_phenx_1r_p
  - Q2 Violence is not a problem in my neighborhood; neighb_phenx_2r_p
  - Q3 My neighborhood is safe from crime.] neighb_phenx_3r_p

```{r message=FALSE, warning=FALSE}
abcd_incl %>% 
  select(neighb_phenx_ss_mean_p) %>% 
  ggplot(aes(x = neighb_phenx_ss_mean_p)) +
  geom_histogram(bins = 12, fill = "white", colour = "black") + 
  xlab("Mean Neighborhood Safety") + 
  ylab("Total (n)")+
  theme_minimal()
```

### Parental income, 
TOTAL COMBINED FAMILY INCOME for the past 12 months

```{r message=FALSE, warning=FALSE}
abcd_incl %>% 
  filter(event_name=="Year 1") %>% 
  select(income_yr1_f) %>% 
  filter(!is.na(income_yr1_f)) %>% 
  group_by(income_yr1_f) %>% 
  summarise(Total = n()) %>% 
  mutate('Proportion %' = Total/sum(Total)*100)

abcd_incl %>%
  filter(event_name=="Year 1") %>%
  select(income_base_f) %>%
  filter(!is.na(income_base_f)) %>% 
  group_by(income_base_f) %>% 
  summarise(Total = n()) %>% 
  mutate('Proportion %' = Total/sum(Total)*100)

abcd_incl %>% 
  filter(event_name == "Baseline", demo_comb_income_v2 != 777, demo_comb_income_v2 != 999) %>% 
  select(demo_comb_income_v2) %>% 
  ggplot(aes(x = demo_comb_income_v2)) + 
  geom_histogram(stat = "count", fill = "white", colour = "black") + 
  xlab("Baseline Self-Reported Income") + 
  ylab("Total (n)")+
  theme_minimal()
```

### Area Deprivation Index (ADI)
Residential History Derived Scores; weighted sum score used to create the percentile rankings described in the pre-reg report (Kind & Buckingham, 2018; doi: 10.1056/NEJMp1802313). Percentile ranking is used to provide a visualization of the weighted sum score in the Neighborhood Altas map. clarify (to reviewers) using the weighted sum score provided in DEAP, not the percentile rankings (not available in DEAP) that are used to visualize the ADI weighted sum score. 

The `Area Deprivation Index` is composed of 17 education, employment, housing-quality, and poverty measures original drawn from census data and updated with recent American Community Survey data. (Kind & Buckingham, 2018). Thus, includes similar limitations as ACS/Census data (limited account of undocumented-immigrant populations).

  - cross reference of over 69 million nine-digit Zip Codes 
  - provides the distribution neighborhood disadvantage throughout us. See map at: (http://www.neighborhoodatlas.medicine.wisc.edu.proxy.lib.umich.edu/)

```{r message=FALSE, warning=FALSE}
abcd_incl %>% 
  select(reshist_addr1_adi_wsum) %>% 
  ggplot(aes(x = reshist_addr1_adi_wsum)) + 
  geom_histogram(bins = 15, fill = "white", colour = "black") + 
  xlab("Baseline Area Deprivation Index") + 
  ylab("Total (n)")+
  theme_minimal()

abcd_incl %>% 
  select(reshist_addr1_adi_wsum) %>% 
  summarise(Min_of_sample = min(reshist_addr1_adi_wsum, na.rm = T), Max_of_sample = max(reshist_addr1_adi_wsum, na.rm = T))
```


## Moderator 
### Parental acceptance

CRPBI - Acceptance Subscale Mean of Report by Parent Completing Protocol by youth: (crpbi_parent1_y + crpbi_parent2_y + crpbi_parent3_y], crpbi_parent4_y + crpbi_parent5_y)/5; Validation: Minimum of 4 items answered; 
[1-not like him/her to 3-a lot like him/her]; 

  - Q1  crpbi_acceptance_studycaregiver1 - Makes me feel better after talking over my worries with him/her; 
  - Q2  crpbi_acceptance_studycaregiver2 - Smiles at me very often; 
  - Q3  crpbi_acceptance_studycaregiver3 - Is able to make me feel better when I am upset. 
  - Q4  crpbi_acceptance_studycaregiver4 - Believes in showing his/her love for me.
  - Q5  crpbi_acceptance_studycaregiver5 - Is easy to talk to.
  
```{r message=FALSE, warning=FALSE}
abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(crpbi_acceptance_ss_studycaregiver) %>% 
  ggplot(aes(x = crpbi_acceptance_ss_studycaregiver)) + 
  geom_histogram(bins = 12, fill = "white", colour = "black") + 
  xlab("Baseline CRPBI Parental Acceptance") + 
  ylab("Total (n)")+
  theme_minimal()
```

## Amyg Activation

### Faces versus placed ROI mean beta

Left & Right Amygdala activation

```{r message=FALSE, warning=FALSE}
#######
#   Rainclouds
######
library(cowplot)
library(dplyr)
library(readr)
library(devtools)
source("https://raw.githubusercontent.com/RainCloudPlots/RainCloudPlots/master/tutorial_R/R_rainclouds.R")

abcd_brain = abcd_incl %>% filter(event_name=='Baseline')




abcd_brain %>% 
  select(Left_amyg_nback, Right_amyg_nback, Bilat_Amyg) %>%
  gather(key = "Region", 
         value = "Beta",
         Left_amyg_nback:Bilat_Amyg) %>% 
  group_by(Region) %>% 
  summarise(Mean = mean(Beta)) %>% 
  ggplot(aes(x = Region, y = Mean, fill =factor(Region))) +
  geom_bar(stat="identity", position = "dodge") +
  ggtitle("Mean Signal Intensity by Region Type: During Nback Faces v. Places") +
  theme_minimal()
  

abcd_brain %>% 
  select(Left_amyg_nback, Right_amyg_nback, Bilat_Amyg) %>%
  gather(key = "Region", 
         value = "Beta",
         Left_amyg_nback:Bilat_Amyg) %>% 
  ggplot(aes(x=Region, y = Beta, fill = Region, colour = Region))+
  geom_flat_violin(position = position_nudge(x = .25, y = 0),adjust =1, trim = FALSE)+ # x - adjusts position of boxplot in relation to distribution. Y displaces distribution, adjust value  modifies distribution type (smoothing kernel)
  geom_point(position = position_jitter(width = .1), size = .1)+ # widtih changes height of points distribution, size changes point size
  #geom_boxplot(aes(x = as.numeric(event_name)+0.25, y = cbcl_scr_syn_internal_t),outlier.shape = NA, alpha = 0.3, width = .1, colour = "BLACK") + # .25 CHANGES POSITIONS OF BOXPLOT
  ylab('Mean Beta Coefficients') +
  xlab('') +
  coord_flip() + # flips axis
  theme_cowplot() + 
  guides(fill = FALSE, colour = FALSE) +
  scale_colour_brewer(palette = "Set2") +
  scale_fill_brewer(palette = "Set2") +
  ggtitle("Distribution of Mean Beta Estimates for Subjects") +
  theme_minimal()

```

## Outcome

### INTERNALIZING SCORE 

ABCD Parent Child Behavior Checklist Scores Aseba (CBCL)

```{r message=FALSE, warning=FALSE}
abcd_incl  %>% 
  filter(event_name=='Baseline') %>% 
  select(cbcl_scr_syn_internal_t) %>% 
  summarise("Responders CBCL Baseline" = n(), Mean = mean(cbcl_scr_syn_internal_t, na.rm = T), SD = sd(cbcl_scr_syn_internal_t, na.rm = T))

abcd_incl  %>% 
  filter(event_name=='Year 1') %>% 
  select(cbcl_scr_syn_internal_t) %>% 
  summarise("Responders CBCL Year 1" = n(), Mean = mean(cbcl_scr_syn_internal_t, na.rm = T), SD = sd(cbcl_scr_syn_internal_t, na.rm = T))

abcd_incl  %>% 
  select(cbcl_scr_syn_internal_t, event_name) %>% 
  ggplot(aes (x= cbcl_scr_syn_internal_t)) +
  geom_histogram(binwidth = 5, fill = "white", colour = "black") + 
  xlab("CBCL Internalize (t)") + 
  ylab("Total (n)")+
  facet_wrap(~event_name)+
  theme_minimal()

### rain clouds
abcd_sample = abcd_incl %>% filter(event_name=='Baseline' | event_name=='Year 1')

abcd_sample %>% 
  ggplot(aes(x=event_name, y = cbcl_scr_syn_internal_t, fill = event_name, colour = event_name))+
  geom_flat_violin(position = position_nudge(x = .25, y = 0),adjust =1, trim = FALSE)+ # x - adjusts position of boxplot in relation to distribution. Y displaces distribution, adjust value  modifies distribution type (smoothing kernel)
  geom_point(position = position_jitter(width = .1), size = .1)+ # widtih changes height of points distribution, size changes point size
  #geom_boxplot(aes(x = as.numeric(event_name)+0.25, y = cbcl_scr_syn_internal_t),outlier.shape = NA, alpha = 0.3, width = .1, colour = "BLACK") + # .25 CHANGES POSITIONS OF BOXPLOT
  ylab('CBCL Internalizing') +
  xlab('Wave') +
  coord_flip() + # flips axis
  theme_cowplot() + 
  guides(fill = FALSE, colour = FALSE) +
  scale_colour_brewer(palette = "Set2") +
  scale_fill_brewer(palette = "Set2") +
  ggtitle("") +
  theme_minimal()

```

## Covariates

### Puberty

```{r message=FALSE, warning=FALSE}
abcd_sample %>% 
  filter(!is.na(sex), event_name == "Baseline") %>% 
  mutate(Sex_r = if_else(sex == "F", "Female", "Male")) %>% 
  ggplot(aes(x=Sex_r, y = puberty, fill = Sex_r, colour = Sex_r))+
  geom_flat_violin(position = position_nudge(x = .25, y = 0),adjust =1, trim = FALSE)+ # x - adjusts position of boxplot in relation to distribution. Y displaces distribution, adjust value  modifies distribution type (smoothing kernel)
  geom_point(position = position_jitter(width = .1), size = .1)+ # widtih changes height of points distribution, size changes point size
  ylab('Pubertal Development') +
  xlab('Sex') +
  coord_flip() + # flips axis
  theme_cowplot() + 
  guides(fill = FALSE, colour = FALSE) +
  scale_colour_brewer(palette = "Set2") +
  scale_fill_brewer(palette = "Set2") +
  ggtitle("") +
  theme_minimal()


abcd_sample %>% 
  filter(!is.na(sex), event_name == "Baseline") %>% 
  mutate(Sex_r = if_else(sex == "F", "Female", "Male")) %>% 
  group_by(Sex_r) %>% 
  summarise(Mean = mean(puberty, na.rm = T), SD = sd(puberty, na.rm = T), Subjs = n())

```

### Scanner Manufacturer

```{r message=FALSE, warning=FALSE}
abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(mri_info_manufacturer) %>% 
  group_by(mri_info_manufacturer) %>% 
  summarize(count = n())


abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(mri_info_manufacturer) %>% 
  group_by(mri_info_manufacturer) %>% 
  summarize(Scanner = n()) %>% 
  ggplot(aes(x = mri_info_manufacturer, y = Scanner, fill =as.factor(mri_info_manufacturer))) +
  geom_bar(stat="identity", position = "dodge") +
  ggtitle("Scanner Types") +
  geom_text(aes(label=Scanner), position=position_dodge(width=0.9), vjust=-0.25)+
  scale_fill_discrete(name="Scanner Type",
                      breaks=c(1:3),
                      labels=c(1:3))+
  theme(axis.text.x=element_text(size=11, angle=30, vjust=.8, hjust=0.8))+
  theme_minimal()
```

### ABCD Site

```{r message=FALSE, warning=FALSE}
abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(abcd_site) %>% 
  group_by(abcd_site) %>% 
  summarize(count = n()) %>% 
  arrange(count)

abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(site) %>% 
  group_by(site) %>% 
  summarize(count = n()) %>% 
  ggplot(aes(x = site, y = count, fill =as.factor(site))) +
  geom_bar(stat="identity", position = "dodge") +
  ggtitle("Total Subjects Per Site") +
  geom_text(aes(label=count), position=position_dodge(width=0.9), vjust=-0.25)+
  scale_fill_discrete(name="ABCD Site",
                      breaks=c(1:22),
                      labels=c(1:22))+
  theme(axis.text.x=element_text(size=11, angle=30, vjust=.8, hjust=0.8))+
  theme_minimal()



abcd %>% 
  filter(event_name == "Baseline") %>% 
  select(site,  exclude) %>% 
  group_by(exclude, site) %>% 
  summarize(count = n()) %>% 
  summarize(count_prop = count/sum(count), count, site) %>% 
  ggplot(aes(x = site, y = count_prop, fill =as.factor(site))) +
  geom_bar(stat="identity", position = "dodge") +
  xlab("ABCD Site") +
  ylab("% included/excluded") +
  ggtitle("Total Subjects Per Site Include/Excluded") +
  facet_wrap(~exclude) +
  geom_text(aes(label=site), position=position_dodge(width=0.9), vjust=-0.25)+
  scale_fill_discrete(name="ABCD Site",
                      breaks=c(1:22),
                      labels=c(1:22))+
  theme(axis.text.x=element_text(size=11, angle=30, vjust=.8, hjust=0.8))+
  theme_minimal()
```

### Age (months) at baseline

```{r message=FALSE, warning=FALSE, include=FALSE}

abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(interview_age) %>% 
  group_by(interview_age) %>% 
  summarise(Count = n())


abcd_sample %>% 
  ggplot(aes(x=event_name, y = interview_age, fill = event_name, colour = event_name))+
  geom_flat_violin(position = position_nudge(x = .25, y = 0),adjust =1, trim = FALSE)+ # x - adjusts position of boxplot in relation to distribution. Y displaces distribution, adjust value  modifies distribution type (smoothing kernel)
  geom_point(position = position_jitter(width = .1), size = .1)+ # widtih changes height of points distribution, size changes point size
  ylab('Age (months)') +
  xlab('Wave') +
  coord_flip() + # flips axis
  theme_cowplot() + 
  guides(fill = FALSE, colour = FALSE) +
  scale_colour_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  ggtitle("") +
  theme_minimal()
```

### Sex 
```{r message=FALSE, warning=FALSE}
abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(sex, subj_id) %>% #subject ID;s
   group_by(sex) %>% # sex at birth
  distinct(subj_id) %>% 
  summarise(count = n())

abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(sex) %>% 
  group_by(sex) %>% 
  summarize(sex_t = n()) %>% 
  ggplot(aes(x = sex, y = sex_t, fill =as.factor(sex_t))) +
  geom_bar(stat="identity", position = "dodge") +
  ggtitle("") +
  ylab("Total") +
  xlab("Sex") +
  geom_text(aes(label=sex_t), position=position_dodge(width=0.9), vjust=-0.25)+
  scale_fill_discrete(name="Sex",
                      breaks=c(1:3),
                      labels=c(1:3))+
  theme(axis.text.x=element_text(size=11, angle=30, vjust=.8, hjust=0.8))+
  theme_minimal()
```

### Race
```{r message=FALSE, warning=FALSE}

# 1: White | 2: Black | 3: Asian | 4: Other/Mixed
abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(race5) %>% 
         group_by(race5) %>% # 
           summarise(count = n())

abcd_incl %>% 
  filter(event_name == "Baseline") %>% 
  select(race5) %>% 
  group_by(race5) %>% 
  summarize(race = n()) %>% 
  ggplot(aes(x = race5, y = race, fill =as.factor(race5))) +
  geom_bar(stat="identity", position = "dodge") +
  ggtitle("") +
  ylab("Total") +
  xlab("Race") +
  geom_text(aes(label=race), position=position_dodge(width=0.9), vjust=-0.25)+
  scale_fill_discrete(name="Race",
                      breaks=c(1:3),
                      labels=c(1:3))+
  theme(axis.text.x=element_text(size=11, angle=30, vjust=.8, hjust=0.8))+
  theme_minimal()

```

### Family relation

rel_family_id: These values are derived from information contained in pairs of participant IDs that are marked as either sibling, twin, or triplet. The family ID is the same for all participants in a family and different between families. Inside a family each set of twins, triplets or siblings will have a unique group ID. A family with 2 sets of twins and one siblings will there have three unique group IDs. 

```{r message=FALSE, warning=FALSE, include=FALSE}
abcd %>% 
  filter(event_name == "Baseline") %>% 
  select(rel_family_id) %>% 
  group_by(rel_family_id) %>% 
  summarise(Count = n())
```


# Tables

## Descriptives: Included/Excluded 
```{r message=FALSE, warning=FALSE}
# table creating using http://thatdatatho.com/2018/08/20/easily-create-descriptive-summary-statistic-tables-r-studio/
library(table1)
library(MatchIt)
table1::label(abcd$interview_age) <- "Age (Months)"
table1::label(abcd$sex) <- "Sex"
table1::label(abcd$income_base_f) <- "Family Income"
table1::label(abcd$race5) <- "Race"
table1::label(abcd$puberty) <- "Puberty"
table1::label(abcd$cbcl_scr_syn_internal_t) <- "Internalizing"
table1::label(abcd$neighb_phenx_ss_mean_p) <- "Neighborhood Safety Mean"
table1::label(abcd$reshist_addr1_adi_wsum) <- "ADI"
table1::label(abcd$crpbi_acceptance_ss_studycaregiver) <- "Parental Acceptance"


table1::table1(~interview_age + sex + income_base_f + race5 + puberty + cbcl_scr_syn_internal_t + neighb_phenx_ss_mean_p + reshist_addr1_adi_wsum + crpbi_acceptance_ss_studycaregiver | as.factor(exclude)*event_name, data = abcd)


#Creating included/excluded subset data for decriptives

table1::label(abcd_incl$interview_age) <- "Age (Months)"
table1::label(abcd_incl$sex) <- "Sex"
table1::label(abcd_incl$income_base_f) <- "Family Income"
table1::label(abcd_incl$race5) <- "Race"
table1::label(abcd_incl$puberty) <- "Puberty"
table1::label(abcd_incl$cbcl_scr_syn_internal_t) <- "Internalizing"
table1::label(abcd_incl$neighb_phenx_ss_mean_p) <- "Neighborhood Safety Mean"
table1::label(abcd_incl$reshist_addr1_adi_wsum) <- "ADI"
table1::label(abcd_incl$crpbi_acceptance_ss_studycaregiver) <- "Parental Acceptance"


table1::table1(~interview_age + sex + income_base_f + race5 + puberty + cbcl_scr_syn_internal_t + neighb_phenx_ss_mean_p + reshist_addr1_adi_wsum + crpbi_acceptance_ss_studycaregiver | as.factor(exclude)*event_name, data = abcd_incl)
```
### Missing data analyses p-value/effect size

```{r message=FALSE, warning=FALSE}
# Effect size function, cohens D
cohens_d <- function(mean_1, mean_2, sd_1, sd_2){
  pooled_sd<- sqrt((sd_1^2+sd_2^2)/2)
  d <- (mean_1 - mean_2)/pooled_sd
  print(paste("cohens d = ",d))
  }
```

### overall sample differences
```{r message=FALSE, warning=FALSE}
# Overall sample
# age
t.test(abcd$interview_age[abcd$event_name=="Baseline"] ~ abcd$exclude[abcd$event_name=="Baseline"])
age <- abcd %>% 
  filter(event_name=="Baseline") %>% 
  group_by(exclude) %>% 
  summarise(M_age = mean(interview_age, na.rm = T), SD_age = sd(interview_age, na.rm = T))

cohens_d(age$M_[1], age$M[2],
         age$SD[1],age$SD[2])

# baseline income
t.test(abcd$income_base_r[abcd$event_name=="Baseline"] ~ abcd$exclude[abcd$event_name=="Baseline"])

income <- abcd %>% 
  filter(event_name=="Baseline") %>% 
  group_by(exclude) %>% 
  summarise(M = mean(income_base_r, na.rm = T), SD = sd(income_base_r, na.rm = T))

cohens_d(income$M[1], income$M[2],
         income$SD[1],income$SD[2])

# Race
library(descr)
CrossTable(abcd$race5[abcd$event_name=="Baseline"], abcd$exclude[abcd$event_name=="Baseline"], chisq = TRUE)
  #chi-square effect sizes Ф = sqrt(chi-square/coun in all cells) ==
sqrt(22.15/11881)


# gender
CrossTable(abcd$sex[abcd$event_name=="Baseline"], abcd$exclude[abcd$event_name=="Baseline"], chisq = TRUE)
  #chi-square effect sizes Ф = sqrt(chi-square/coun in all cells) ==
sqrt(380.4872/11881)


# baseline puberty
t.test(abcd$puberty[abcd$event_name=="Baseline"] ~ abcd$exclude[abcd$event_name=="Baseline"])

puberty <- abcd %>% 
  filter(event_name=="Baseline") %>% 
  group_by(exclude) %>% 
  summarise(M = mean(puberty, na.rm = T), SD = sd(puberty, na.rm = T))

cohens_d(puberty$M[1], puberty$M[2],
         puberty$SD[1],puberty$SD[2])

# baseline internalizing
t.test(abcd$cbcl_scr_syn_internal_t[abcd$event_name=="Baseline"] ~ abcd$exclude[abcd$event_name=="Baseline"])

cbcl <- abcd %>% 
  filter(event_name=="Baseline") %>% 
  group_by(exclude) %>% 
  summarise(M = mean(cbcl_scr_syn_internal_t, na.rm = T), SD = sd(cbcl_scr_syn_internal_t, na.rm = T))


cohens_d(cbcl$M[1], cbcl$M[2],
         cbcl$SD[1],cbcl$SD[2])

# baseline Neighb SAFE
t.test(abcd$neighb_phenx_ss_mean_p[abcd$event_name=="Baseline"] ~ abcd$exclude[abcd$event_name=="Baseline"])

Neigh_safe <- abcd %>% 
  filter(event_name=="Baseline") %>% 
  group_by(exclude) %>% 
  summarise(M = mean(neighb_phenx_ss_mean_p, na.rm = T), SD = sd(neighb_phenx_ss_mean_p, na.rm = T))


cohens_d(Neigh_safe$M[1], Neigh_safe$M[2],
         Neigh_safe$SD[1],Neigh_safe$SD[2])


# baseline ADI
t.test(abcd$reshist_addr1_adi_wsum[abcd$event_name=="Baseline"] ~ abcd$exclude[abcd$event_name=="Baseline"])

ADI <- abcd %>% 
  filter(event_name=="Baseline") %>% 
  group_by(exclude) %>% 
  summarise(M = mean(reshist_addr1_adi_wsum, na.rm = T), SD = sd(reshist_addr1_adi_wsum, na.rm = T))


cohens_d(ADI$M[1], ADI$M[2],
         ADI$SD[1],ADI$SD[2])

# baseline Acceptance
t.test(abcd$crpbi_acceptance_ss_studycaregiver[abcd$event_name=="Baseline"] ~ abcd$exclude[abcd$event_name=="Baseline"])

par_accept <- abcd %>% 
  filter(event_name=="Baseline") %>% 
  group_by(exclude) %>% 
  summarise(M = mean(crpbi_acceptance_ss_studycaregiver, na.rm = T), SD = sd(crpbi_acceptance_ss_studycaregiver, na.rm = T))


cohens_d(par_accept$M[1], par_accept$M[2],
         par_accept$SD[1],par_accept$SD[2])



# DIFFERENCE Puberty-by-gender 
t.test(abcd_incl$puberty[abcd_incl$event_name=="Baseline"] ~ abcd_incl$sex[abcd_incl$event_name=="Baseline"])

pubert_sex <- abcd_incl %>% 
  filter(event_name=="Baseline") %>% 
  group_by(sex) %>% 
  summarise(M = mean(puberty, na.rm = T), SD = sd(puberty, na.rm = T))


cohens_d(pubert_sex$M[1], pubert_sex$M[2],
         pubert_sex$SD[1],pubert_sex$SD[2])
```

To interpret easily latent factor in SEM

## Correl (pearsons r)

### Recoding/renaming

recoding for ease of interpretation in correlations, SEM (loading factors all pos/neg), and shorterning names

```{r message=FALSE, warning=FALSE}
## Creating numeric ID: 
set.seed(200)
abcd_incl$IDs <- as.numeric(as.factor(abcd_incl$subj_id))

# shortening names
abcd_mplus <- abcd_incl %>% 
  rename(ID = IDs,
         Event = event_name,
         Age = interview_age,
         Sex_M1F0 = sex_r,
         Race = ethnicity,
         Inc_BL = income_base_r,
         Puberty = puberty,
         ADI = reshist_addr1_adi_wsum,
         CBCL_t = cbcl_scr_syn_internal_t,
         Neigh_safe = neighb_phenx_ss_mean_p,
         Par_Accept = crpbi_acceptance_ss_studycaregiver,
         L_Amyg = Left_amyg_nback, 
         R_Amyg = Right_amyg_nback,
         Family = rel_family_id,
         ABCDsite = site) 

```

### Recoding baseline income/SR parental safety

New baseline income `income_base_recoded`
new reverse coded: `neighb_phenx_ss_mean_p_recode`

```{r message=FALSE, warning=FALSE}
library(car)
abcd_mplus = abcd_mplus %>% 
  mutate(Inc_BL_r = -1*Inc_BL + 11)

abcd_mplus = abcd_mplus %>% 
  mutate(Neigh_safe_r = -1*Neigh_safe + 5)


```

### table/plot  

```{r message=FALSE, warning=FALSE}
abcd_base = abcd_mplus %>% 
  filter(Event == "Baseline")

correl = data.frame(Age = abcd_base$Age, 
                    Sex = abcd_base$Sex_M1F0, 
                    Puberty = abcd_base$Puberty, 
                    Income = abcd_base$Inc_BL_r, 
                    ADI = abcd_base$ADI, 
                    "Neigh Safe" = abcd_base$Neigh_safe_r, 
                    Accept = abcd_base$Par_Accept, 
                    "CBCL_t" = abcd_base$CBCL_t, 
                    "L Amyg" = abcd_base$L_Amyg, 
                    "R Amyg" = abcd_base$R_Amyg,
                    "Bilat Amyg" = abcd_base$Bilat_Amyg,
                    "Nback FD" = abcd_base$nback_meanFD_motion
                    )

cor_table = cor(correl, use = "pairwise", method = "pearson")

library(kableExtra) # for table formatting
knitr::kable(cor_table, digits = 2) %>% 
  kable_styling(full_width = T)


library(corrplot)
corrplot(cor_table, type = "upper", 
         method = "pie", #title = "Pairwise Pearson r among variables", 
         tl.cex = 0.7, tl.col = 'black',
         mar=c(1, 1, 1, 1))

# write.csv(cor_table, "~/Downloads/correlation_vars.csv")
```


# Prep/Export: Mplus 

## Wide form, Var select
```{r message=FALSE, warning=FALSE}

# BL and Y1 rather than 1 and 2 so that it was clear when it was combined with the variable names 
Mplus_data = abcd_mplus %>% 
  mutate(Event_short = if_else(Event == "Baseline", "BL", "Y1"))

# dummy code scan variable
Mplus_data = Mplus_data %>% 
  mutate(Scan1 = if_else(scanner_type == 1, 1, 0), 
         Scan2 = if_else(scanner_type == 2, 1, 0))

vars = c("Age","Event","Sex_M1F0","Race","Inc_BL","Puberty","ADI","CBCL_t",
         "Neigh_safe","Par_Accept","L_Amyg","R_Amyg","Bilat_Amyg",
         "Family", "Inc_BL_r","Neigh_safe_r",
         "scanner_type","Scan1","Scan2","ABCDsite")
# wide format
Mplus_wide <- as.data.frame(pivot_wider(data = Mplus_data,
                               id_cols = ID,
                               names_from = Event_short,
                               names_sep = "_",
                               values_from = vars
                                 )
                                )


```

## Marking NA values
Replacing `NA` with `99999` 
```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}

# Removing non-existent columns (e.g. L_Amyg_Y1) and cleaning up variable names
all_na <- function(x) any(!is.na(x))

Mplus_wide = Mplus_wide %>% 
  select_if(all_na) %>% 
  rename(Inc_BL = Inc_BL_BL,
         Inc_BL_r = Inc_BL_r_BL,
         Family = Family_BL)

Mplus_wide[Mplus_wide=='NULL'] <- 99999  
Mplus_wide[is.na(Mplus_wide)] = 99999

```

## Saving `.dat` file
```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
library(multiplex)
library(MplusAutomation)

Mplus_wide_final_SENSITIVITY <- Mplus_wide %>% 
  filter(Event_BL =="Baseline") %>% 
  select(-Event_BL, -Event_Y1)

write.dat(Mplus_wide_final_SENSITIVITY, "~/Desktop/UM/4_ABCD/3_Projects/1_Cortex_sub/stage2/data/Cortex_Mplus_2020_12_09_WIDE_SENSITIVITY.dat")
colnames(Mplus_wide_final_SENSITIVITY)

```

